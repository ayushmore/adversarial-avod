---
layout: default
title: Adversarial Training in Deep Fusion Modeling
---
<div class="blurb">
	<h1>Adversarial Training in Deep Fusion Modeling in Autonomous Driving</h1>
	<p>Amy Nguyen & Ayush More</p>
	<h2>Introduction</h2>
	<p>Deep fusion models benefit from multiple input sources as they provide both complementary information. This enriches the model’s feature space, and shared information, which compensates for the shortcomings of different sources. Robustness in deep fusion models is primarily associated with shared information as the input sources face varying corruption and it is important for the model to be balanced in its performance regardless of the corruption. Kim, Taewan, and Joydeep Ghosh's [1] work highlights the motivation of their research through showcasing unbalanced robustness in an error-free model for linear fusion data. To address this issue, they provide three different approaches, two of which were training algorithms and the other implemented a structural change to the model's feature fusion. By focusing on autonomous driving, the authors experimented with a deep fusion object detection model, AVOD, and found their methods to be effective in developing robustness. In the current setting, the noise is generated randomly from a Gaussian distribution rather than a targeted attack, which does not account for intentional corruption from third parties, such as hackers. We plan on expanding on their research by incorporating adversarial examples, which are images with noise masked over each pixel that causes a model to make a mistake in labeling. These are slightly perturbed  into the training algorithm rather than use images perturbed by random noise. The AVOD model uses two inputs to create its inferences, images and LIDAR point clouds. By creating subsets of adversarial examples from the image data, we will train the model on the corrupted data and evaluate its robustness against single source noise. We believe that this is relevant given the risks that autonomous vehicles pose to pedestrians - it is important that we ensure the inferences and decisions made by the model are robust against corruption, especially if it is intentional from outside threats.</p>
	<h2>Data</h2>
	<p>KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. Álvarez et al. generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. Zhang et al. annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.</p>
	<h2>Adversarial Examples</h2>
	<p>As mentioned in the background, an adversarial example is an image with noise masked over each pixel that causes a model to make a mistake in labeling. Since an adversarial example’s predicted output should fail to match the true label, creating an adversarial example differs from the typical way of training a classifier. The usual way would be to minimize the loss of the input’s predicted output to the true output as represented by...</p>
	<h2>Training Algorithm</h2>
	<p>Check out how we did this!</p>
	<h2>Results</h2>
	<p>Check out our results!</p>
	<h2>Conclusion</h2>
	<p>This is what we ended up with</p>
</div><!-- /.blurb -->
