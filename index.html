---
layout: default
title: Adversarial Training in Deep Fusion Modeling
---
<div class="blurb">
	<h1>Adversarial Training in Deep Fusion Modeling in Autonomous Driving</h1>
	<p>Amy Nguyen & Ayush More</p>
	<h2>Introduction</h2>
	<p>Deep fusion models benefit from multiple input sources as they provide both complementary information. This enriches the model’s feature space, and shared information, which compensates for the shortcomings of different sources. Robustness in deep fusion models is primarily associated with shared information as the input sources face varying corruption and it is important for the model to be balanced in its performance regardless of the corruption. Kim, Taewan, and Joydeep Ghosh's [1] work highlights the motivation of their research through showcasing unbalanced robustness in an error-free model for linear fusion data. To address this issue, they provide three different approaches, two of which were training algorithms and the other implemented a structural change to the model's feature fusion. By focusing on autonomous driving, the authors experimented with a deep fusion object detection model, AVOD, and found their methods to be effective in developing robustness. In the current setting, the noise is generated randomly from a Gaussian distribution rather than a targeted attack, which does not account for intentional corruption from third parties, such as hackers. We plan on expanding on their research by incorporating adversarial examples, which are images with noise masked over each pixel that causes a model to make a mistake in labeling. These are slightly perturbed  into the training algorithm rather than use images perturbed by random noise. The AVOD model uses two inputs to create its inferences, images and LIDAR point clouds. By creating subsets of adversarial examples from the image data, we will train the model on the corrupted data and evaluate its robustness against single source noise. We believe that this is relevant given the risks that autonomous vehicles pose to pedestrians - it is important that we ensure the inferences and decisions made by the model are robust against corruption, especially if it is intentional from outside threats.</p>
	<h2>Data</h2>
	<p>KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. Álvarez et al. generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. Zhang et al. annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.</p>
	<h2>Adversarial Examples</h2>
	<p>An adversarial example is an image with noise masked over each pixel that causes a model to make a mistake in labeling. Since an adversarial example’s predicted output should fail to match the true label, creating an adversarial example differs from the typical way of training a classifier. The usual way would be to minimize the loss of the input’s predicted output to the true output as represented by...</p>
	<h2>Training Algorithm</h2>
	<p>We will implement our adversarial training algorithm through developing adversarial examples from the input sources and optimizing the maximum perturbation added to the data. First, we will find the best input source to perturb as determined by the largest loss we observe based on random noise perturbation and calculate the best perturbation to add by using FGSM. Then, we plan on repeating this procedure for a defined percentage of the dataset so that we can populate our training set with these adversarial examples. Ultimately, this should make the model more robust against adversarial attacks in one of its many input sources.</p>
	<h2>Results</h2>
	<p>From our research project, we plan on observing to see if we were able to make the model more robust through implementing the different adversarial attack techniques instead of the randomly generated noise on a single source. This is relevant because we further explore attacks that are targeted towards a certain class through selecting the most perturbation to an image, which pose a more serious risk than randomly corrupting an input source. We plan on communicating our findings through a research paper and expanding on the work done by Kim, Taewan, and Joydeep Ghosh.</p>
	<h2>Conclusion</h2>
	<p>This is what we ended up with!</p>
</div><!-- /.blurb -->
